Title: 

URL Source: https://arxiv.org/pdf/2402.07927

Markdown Content:
# A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications 

Pranab Sahoo 1 , Ayush Kumar Singh 1 , Sriparna Saha 1 , Vinija Jain 2,3 , Samrat Mondal 1 and Aman Chadha 2,31Department of Computer Science And Engineering, Indian Institute of Technology Patna 

> 2

Stanford University, 3Amazon AI 

{pranab_2021cs25, ayush_2211ai27, sriparna, samrat}@iitp.ac.in, hi@vinija.ai, hi@aman.ai 

Abstract 

Prompt engineering has emerged as an indispens-able technique for extending the capabilities of large language models (LLMs) and vision-language mod-els (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model param-eters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vec-tor representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and tech-niques. This survey paper addresses the gap by pro-viding a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limita-tions of each approach and include a taxonomy dia-gram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates fu-ture research by illuminating open challenges and opportunities for prompt engineering. 

1 Introduction 

Prompt engineering has emerged as a crucial technique for enhancing the capabilities of pre-trained large language mod-els (LLMs) and vision-language models (VLMs). It involves strategically designing task-specific instructions, referred to as prompts, to guide model output without altering parameters. The significance of prompt engineering is especially evident in its transformative impact on the adaptability of LLMs and Prompt Engineering 

> Instruction
> User's Input
> LLM
> Pre-trained on
> billions of
> parameters
> Output: Response
> generated by LLM Context

Figure 1: Visual breakdown of prompt engineering components: LLMs trained on extensive data, instruction and context as pivotal elements shaping the prompt, and a user input interface. 

VLMs. By offering a mechanism to fine-tune model outputs through carefully crafted instructions, prompt engineering en-ables these models to excel across diverse tasks and domains. This adaptability is different from traditional paradigms, where model retraining or extensive fine-tuning is often required for task-specific performance. This is the transformative promise of prompt engineering, pushing the boundaries of AI and opening doors to a future brimming with possibilities. In an ever-evolving landscape, ongoing research consistently re-veals innovative approaches and applications within prompt engineering. The significance of prompt engineering is under-scored by its capacity to steer model responses, enhancing the adaptability and applicability of LLMs across diverse sectors. The landscape of contemporary prompt engineering spans a spectrum of techniques, encompassing foundational methods like zero-shot and few-shot prompting to more intricate ap-proaches such as "chain of code" prompting. The notion of prompt engineering was initially investigated and popularized in the LLMs [Liu et al. , 2023 ] , [ Tonmoy et al. , 2024 ], [ Chen 

et al. , 2023 ] later extended to VLMs [Wu et al. , 2023 ] , [ Bahng 

et al. , 2022 ] . Despite the extensive literature on prompt engi-neering within both LLMs and VLMs, a notable gap remains, particularly concerning a systematic overview of application-centric prompt engineering techniques. With recent strides in prompt engineering, there is a pressing need for a com-prehensive survey that offers a nuanced understanding of ap-plications and advancements in contemporary research. This survey dives deep into the ever-evolving landscape of prompt engineering, analyzing over 29 distinct techniques categorized by their diverse applications. Employing a systematic review approach, we meticulously delve into the intricacies of diverse 

> arXiv:2402.07927v1 [cs.AI] 5 Feb 2024

cutting-edge prompting methods. Our examination encom-passes their applications, the language models utilized, and the datasets subjected to experimentation, providing a detailed and nuanced analysis of the evolving landscape of prompt en-gineering. Additionally, we discuss the pros and cons of these techniques, offering insights into their comparative efficacy. We unveil a comprehensive taxonomy diagram that sheds light on how these techniques navigate the vast terrain of LLM capabilities. From language generation and question answer-ing to code creation and reasoning tasks, prompt engineering empowers the LLMs into performing feats we never thought possible. By bridging the existing gap in the literature, this survey aims to serve as a valuable resource for researchers and practitioners, offering insights into the latest developments and facilitating a deeper understanding of the evolving land-scape of prompt engineering. The structure of the paper is organized as follows: Section 2 presents the prompt engineer-ing techniques from both basic to advanced by categorizing application-area and Section 3 provides a conclusion along with considerations for future research endeavors. 

2 Prompt Engineering 

In this section, we have organized prompt engineering tech-niques according to their application areas and provided a concise overview of the evolution of prompting techniques, spanning from zero-shot prompting to the latest advancements. 

2.1 New Tasks Without Extensive Training 

Zero-Shot Prompting 

Zero-shot prompting offers a paradigm shift in leveraging large LLMs. This technique [Radford et al. , 2019 ] removes the need for extensive training data, instead relying on carefully crafted prompts that guide the model toward novel tasks. Specifically, the model receives a task description in the prompt but lacks la-beled data for training on specific input-output mappings. The model then leverages its pre-existing knowledge to generate predictions based on the given prompt for the new task. 

Few-Shot Prompting 

Few-shot prompting provides models with a few input-output examples to induce an understanding of a given task, unlike zero-shot prompting, where no examples are supplied [ Brown 

et al. , 2020 ] . Providing even a few high-quality examples has improved model performance on complex tasks compared to no demonstration. However, few-shot prompting requires additional tokens to include the examples, which may become prohibitive for longer text inputs. Moreover, the selection and composition of prompt examples can significantly influ-ence model behavior, and biases like favoring frequent words may still affect few-shot results. While few-shot prompting enhances capabilities for complex tasks, especially among large pre-trained models like GPT-3, careful prompt engineer-ing is critical to achieve optimal performance and mitigate unintended model biases. 

2.2 Reasoning and Logic 

Chain-of-Thought (CoT) Prompting 

LLMs often stumble in the face of complex reasoning, lim-iting their potential. Aiming to bridge this gap, [Wei et al. ,2022 ] introduced Chain-of-Thought (CoT) prompting as a technique to prompt LLMs in a way that facilitates coherent and step-by-step reasoning processes. The primary contribu-tion lies in the proposal and exploration of CoT prompting, demonstrating its effectiveness in eliciting more structured and thoughtful responses from LLMs compared to traditional prompts. Through a series of experiments, the authors show-case the distinctive qualities of CoT prompting, emphasizing its ability to guide LLMs through a logical reasoning chain. This results in responses that reflect a deeper understanding of the given prompts. For example, the prompt would show the reasoning process and final answer for a multi-step math word problem and mimic how humans break down problems into logical intermediate steps. The authors achieved state-of-the-art performance in math and commonsense reasoning benchmarks by utilizing CoT prompts for PaLM 540B, achiev-ing an accuracy of 90.2%. 

Automatic Chain-of-Thought (Auto-CoT) Prompting 

Manual creation of high-quality CoT examples is both time-consuming and suboptimal. [Zhang et al. , 2022 ] introduced Auto-CoT to automatically instruct LLMs with a "Letâ€™s think step-by-step" prompt to generate reasoning chains. Recognizing the possibility of errors in individually generated chains, Auto-CoT enhances robustness through diverse sampling. It sam-ples various questions and generates multiple distinct reasoning chains for each, forming a final set of demonstrations. This automated diverse sampling minimizes errors and enhances few-shot learning, eliminating the need for labor-intensive manual creation of reasoning chains. Auto-CoT demonstrated enhanced performance, surpassing the CoT paradigm with average ac-curacy improvements of 1.33% and 1.5% on arithmetic and symbolic reasoning tasks, respectively, employing GPT-3. 

Self-Consistency 

[Wang et al. , 2022 ] introduced self-consistency, a decoding strategy enhancing reasoning performance compared to greedy decoding in CoT prompting. For complex reasoning tasks with multiple valid paths, self-consistency generates diverse reason-ing chains by sampling from the language modelâ€™s decoder. It then identifies the most consistent final answer by marginalizing these sampled chains. This approach capitalizes on the obser-vation that problems requiring thoughtful analysis often entail greater reasoning diversity, leading to a solution. The combina-tion of self-consistency and chain-of-thought prompting results in significant accuracy improvements across various bench-marks, such as 17.9% on GSM8K, 11.0% on SVAMP, 12.2% on AQuA, 6.4% on StrategyQA, and 3.9% on ARC-challenge compared to the baseline chain-of-thought prompting. 

Logical Chain-of-Thought (LogiCoT) Prompting 

The ability to perform logical reasoning is critical for LLMs to solve complex, multi-step problems across diverse domains. Existing methods, like CoT prompting, encourage step-by-step reasoning but lack effective verification mechanisms. [Zhao 

et al. , 2023 ] proposes a Logical Chain-of-Thought (LogiCoT) prompting, a neurosymbolic framework that leverages princi-ples from symbolic logic to enhance reasoning in a coherent and structured manner. Specifically, LogiCoT applies the con-cept of reductio ad absurdum to verify each step of reasoning Prompt Engineering 

New Tasks Without Extensive Training Â§2.1 

Zero-shot Prompting [Radford et al. , 2019] 

Few-shot Prompting [Brown et al. , 2020] 

Reasoning and Logic Â§2.2 

Chain-of-Thought (CoT) Prompting [Wei et al. , 2022] 

Automatic Chain-of-Thought (Auto-CoT) [Zhang et al. , 2022] 

Self-Consistency [Wang et al. , 2022] 

Logical CoT (LogiCoT) Prompting [Zhao et al. , 2023] 

Chain-of-Symbol (CoS) Prompting [Hu et al. , 2023] 

Tree-of-Thoughts (ToT) Prompting [Yao et al. , 2023a] 

Graph-of-Thought (GoT) Prompting [Yao et al. , 2023b] 

System 2 Attention Prompting [Weston and Sukhbaatar, 2023] 

Thread of Thought (ThoT) Prompting [Zhou et al. , 2023] 

Chain of Table Prompting [Wang et al. , 2024] 

Reduce Hallucination Â§2.3 

Retrieval Augmented Generation (RAG) [Lewis et al. , 2020] 

ReAct Prompting [Yao et al. , 2022] 

Chain-of-Verification (CoVe) [Dhuliawala et al. , 2023] 

Chain-of-Note (CoN) Prompting [Yu et al. , 2023] 

Chain-of-Knowledge (CoK) Prompting [Li et al. , 2023d] 

User Interaction Â§2.4 Active-Prompt [Diao et al. , 2023] 

Fine-Tuning and Optimization Â§2.5 Automatic Prompt Engineer (APE) [Zhou et al. , 2022] 

Knowledge-Based Reasoning and Generation Â§2.6 

Automatic Reasoning and Tool-use (ART) [Paranjape et al. , 2023] 

Improving Consistency and Coherence Â§2.7 

Contrastive Chain-of-Thought Prompting (CCoT) [Chia et al. , 2023] 

Managing Emotions and Tone Â§2.8 Emotion Prompting [Li et al. , 2023a] 

Code Generation and Execution Â§2.9 

Scratchpad Prompting [Nye et al. , 2021] 

Program of Thoughts (PoT) Prompting [Chen et al. , 2022] 

Structured Chain-of-Thought (SCoT) Prompting [Li et al. , 2023c] 

Chain of Code (CoC) Prompting [Li et al. , 2023b] 

Optimization and Efficiency Â§2.10 Optimization by Prompting [Yang et al. , 2023] 

Understanding User Intent Â§2.11 Rephrase and Respond (RaR) Prompting [Deng et al. , 2023] 

Metacognition and Self-Reflection Â§2.12 Take a Step Back Prompting [Zheng et al. , 2023] 

Figure 2: Taxonomy of prompt engineering techniques in LLMs, organized around application domains, providing a nuanced framework for customizing prompts across diverse contexts. 

generated by the model and provide targeted feedback to revise incorrect steps. LogiCoT can reduce logical errors and hallu-cinations through a think-verify-revise loop. Experimenting with Vicuna-33b and GPT-4, the findings underscore Logi-CoTâ€™s notable enhancement of reasoning abilities, exhibiting improvements of 0.16% and 1.42% on the GSM8K dataset and 3.15% and 2.75% on the AQuA dataset compared to CoT, respectively. 

Chain-of-Symbol (CoS) Prompting 

LLMs often struggle with tasks involving complex spatial re-lationships due to their reliance on natural language, which is susceptible to ambiguity and biases. To overcome this limita-tion, [Hu et al. , 2023 ] introduced CoS, employing condensed symbols instead of natural language. CoS provides distinct advantages: clear and concise prompts, heightened spatial rea-soning for LLMs, and improved human interpretability. CoS suffers from challenges such as scalability, generalizability, in-tegration with other techniques, and interpretability of LLM reasoning based on symbols. Notably, the implementation of CoS significantly elevates ChatGPTâ€™s performance, boosting accuracy from 31.8% to an impressive 92.6% on Brick World tasks. Moreover, CoS achieves up to a 65.8% reduction in prompt tokens, streamlining the process while maintaining high accuracy. 

Tree-of-Thoughts (ToT) Prompting 

[Yao et al. , 2023a ] and [Long, 2023 ] proposed the Tree-of-Thoughts (ToT) framework to enhance prompting capabilities for complex tasks requiring exploration and look-ahead reason-ing. ToT extends CoT prompting by managing a tree structure of intermediate reasoning steps, known as "thoughts". Each thought represents a coherent language sequence moving to-ward the final solution. This structure allows language models to deliberately reason by assessing the progress generated by thoughts in solving the problem. ToT integrates the modelâ€™s abil-ities to produce and evaluate thoughts with search algorithms like breadth-first or depth-first search. This enables system-atic exploration among reasoning chains, with a look-ahead to expand promising directions and to backtrack when solutions are incorrect. ToT excelled in the Game of 24 tasks, achieving a 74% success rate compared to CoTâ€™s 4%. Additionally, in word-level tasks, ToT outperformed CoT with a 60% success rate versus 16%. 

Graph-of-Thoughts (GoT) Prompting 

The inherent non-linear nature of human thought processes chal-lenges the conventional sequential approach of CoT prompt-ing. [Yao et al. , 2023b ] introduced the "Graph of Thoughts" prompting, a graph-based framework advancing traditional se-quential methods to better align with the non-linear character-istics of human thinking. This framework permits dynamic interplay, backtracking, and evaluation of ideas, allowing the aggregation and combination of thoughts from various branches, departing from the linear structure of the tree of thoughts. The key contributions encompass modeling the reasoning process as a directed graph, offering a modular architecture with diverse transformation operations. The framework is presented as a ver-satile and dynamic approach to language model prompting, cap-turing the intricacies of human thought processes and enhancing model capabilities. The GoT reasoning model demonstrates substantial gains over the CoT baseline, improving accuracy by 3.41% with T5-base and 5.08% with T5-large on GSM8K. It also boosts accuracy over the state-of-the-art Multimodal-CoT by 6.63% using T5-base and 1.09% with T5-large on ScienceQA. 

System 2 Attention (S2A) Prompting 

The soft attention mechanism in Transformer-based LLMs is prone to incorporating irrelevant context information, impact-ing token generation adversely. To address this, [Weston and Sukhbaatar, 2023 ] proposed System 2 Attention (S2A), utilizing the reasoning abilities of LLMs to selectively attend to relevant portions by regenerating the input context. S2A employs a two-step process to enhance attention and response quality by employing context regeneration and response generation with refined context. The effectiveness of S2A is evaluated across various tasks, including factual QA, long-form generation, and math word problems. In factual QA, S2A attains an accuracy of 80.3%, demonstrating a substantial enhancement in factuality. In long-form generation, it improves objectivity and receives a score of 3.82 out of 5. 

Thread of Thought (ThoT) Prompting 

[Zhou et al. , 2023 ] presented Thread of Thought (ThoT), a prompting technique designed to enhance the reasoning abil-ities of LLMs within chaotic contexts. ThoT, inspired by hu-man cognition, systematically examines extensive contexts into manageable segments for incremental analysis, employing a two-phase approach where the LLM first summarizes and ex-amines each segment before refining the information for a final response. ThoTâ€™s flexibility shines as a versatile "plug-and-play" module, enhancing reasoning across different models and prompting methods. Evaluations on question answering and conversation datasets reveal substantial performance improve-ments of 47.20% and 17.8%, respectively, especially in chaotic contexts. 

Chain-of-Table Prompting 

Approaches like CoT, PoT, and ToT represent reasoning steps through free-form text or code, which face challenges when dealing with intricate table scenarios. The study by [Wang et al. , 2024 ] introduced a pioneering prompting technique named Chain-of-Table. This method uses step-by-step tabular reason-ing by dynamically generating and executing common SQL/-DataFrame operations on tables. The iterative nature of this process enhances intermediate results, empowering LLMs to make predictions through logically visualized reasoning chains. Significantly, Chain-of-Table consistently improves the perfor-mance of two benchmark tabular datasets by 8.69% on TabFact and 6.72% on WikiTQ, respectively. 

2.3 Reduce Hallucination 

Retrieval Augmented Generation (RAG) 

LLMs have revolutionized text generation, yet their reliance on limited, static training data hinders accurate responses, especially in tasks demanding external knowledge. Traditional prompting falls short, requiring expensive retraining. Retrieval Augmented Generation (RAG) [ Lewis et al. , 2020 ] emerges as a novel solution, seamlessly weaving information retrieval into the prompting process. RAG analyzes user input, crafts a targeted query, and scours a pre-built knowledge base for relevant resources. Retrieved snippets are incorporated into the original prompt, enriching it with contextual background. The augmented prompt empowers the LLM to generate creative, factually accurate responses. RAGâ€™s agility overcomes static limitations, making it a game-changer for tasks requiring up-to-date knowledge. RAG outperformed seq2seq models and task-specific architectures on ODQA benchmarks, achieving exact match scores, reaching up to 56.8% on TriviaQA and 44.5% on Natural Questions. 

ReAct Prompting 

Unlike previous studies that treated reasoning and action sep-arately, ReAct [Yao et al. , 2022 ] enables LLMs to generate reasoning traces and task-specific actions concurrently. This interleaved process enhances synergy between reasoning and action, facilitating the model in inducing, tracking, and updat-ing action plans while handling exceptions. ReAct is applied to diverse language and decision-making tasks, showcasing its effectiveness over state-of-the-art baselines. Notably, in question answering (HotpotQA) and fact verification (Fever), ReAct addresses hallucination and error propagation issues by interacting with a simple Wikipedia API, producing more inter-pretable task-solving trajectories. Additionally, in interactive decision-making benchmarks like ALFWorld and WebShop, ReAct surpasses both imitation and reinforcement learning ap-proaches, achieving notable success rates of 34% and 10%, respectively, with minimal in-context examples. 

Chain-of-Verification (CoVe) Prompting 

To address hallucinations in LLMs, [Dhuliawala et al. , 2023 ]proposed Chain-of-Verification (CoVe), which involves a sys-tematic four-step process including the model generate baseline responses, plan verification questions to check its work, answer the questions independently, and produce a revised response incorporating the verification. By verifying its work through this deliberate multi-step approach, the LLM enhances logi-cal reasoning abilities and reduces errors even with contradic-tory information. CoVe emulates human verification to bolster the coherence and precision of LLM output. Experiments on list questions, QA, and long-form generation demonstrate that CoVe decreases hallucinations while maintaining facts. Fo-cused verification questions help models identify and correct their inaccuracies. 

Chain-of-Note (CoN) Prompting 

Retrieval-augmented language models (RALMs) enhance large language models by incorporating external knowledge to reduce factual hallucination. However, the reliability of retrieved information is not guaranteed, leading to potentially misguided responses. Standard RALMs struggle to assess their knowledge adequacy and often fail to respond with "unknown" when lacking information. To address these challenges, [Yu 

et al. , 2023 ] introduced a novel approach to improve RALMs robustness by handling noisy, irrelevant documents and ac-curately addressing unknown scenarios. CoN systematically evaluates document relevance, emphasizing critical and re-liable information to filter out irrelevant content, resulting in more precise and contextually relevant responses. Test-ing across diverse open-domain question-answering datasets demonstrated notable improvements, including a +7.9 average boost in exact match scores for noisy retrieved documents and a +10.5 enhancement in rejection rates for questions beyond pre-training knowledge. 

Chain-of-Knowledge (CoK) Prompting 

Traditional prompting techniques for LLMs have proven power-ful in tackling basic tasks. However, their efficacy diminishes due to complex reasoning challenges, often resulting in unre-liable outputs plagued by factual hallucinations and opaque thought processes. This limitation arises from their reliance on fixed knowledge sources, ineffective structured query gen-eration, and lack of progressive correction that fails to guide the LLM adequately. Motivated by human problem-solving, CoK [Li et al. , 2023d ] systematically breaks down intricate tasks into well-coordinated steps. The process initiates with a comprehensive reasoning preparation stage, where the context is established, and the problem is framed. Subsequently, it en-gages in a dynamic knowledge adaptation phase, meticulously gathering evidence from various sources, such as its internal knowledge base, external databases, and the given prompt. 

2.4 User Interface 

Active Prompting 

[Diao et al. , 2023 ] introduced Active-Prompting as a solution to the challenge of adapting LLMs to diverse reasoning tasks. They address the issue by proposing Active-Prompt to enhance LLMsâ€™ performance on complex question-and-answer tasks through task-specific example prompts with chain-of-thought (CoT) reasoning. Unlike existing CoT methods that rely on fixed sets of human-annotated exemplars, Active-Prompt intro-duces a mechanism for determining the most impactful ques-tions for annotation. Drawing inspiration from uncertainty-based active learning, the method utilizes various metrics to characterize uncertainty and selects the most uncertain ques-tions for annotation. Active-Prompting exhibits superior per-formance, outperforming self-consistency by an average of 7.0% and 1.8% across eight complex reasoning tasks in text-davinci-002 and code-davinci-002, respectively, showcasing state-of-the-art results. 

2.5 Fine-Tuning and Optimization 

Automatic Prompt Engineer (APE) 

While crafting effective prompts for LLMs has traditionally been a laborious task for expert annotators, [Zhou et al. ,2022 ] introduced Automatic Prompt Engineer (APE) as an innovative approach to automatic instruction generation and selection for LLMs. APE sheds the limitations of static, hand-designed prompts by dynamically generating and selecting the most impactful prompts for specific tasks. This ingenious method analyzes user input, crafts candidate instructions, and then leverages reinforcement learning to choose the optimal prompt, adapting it on the fly to different contexts. Extensive tests on the diverse BIG-Bench suite and the CoT reason-ing task revealed APEâ€™s prowess, exceeding human-authored prompts in most cases (19 out of 24 tasks) and significantly boosting LLMs reasoning abilities. This breakthrough in auto-matic prompt engineering paves the way for LLMs to tackle a wider range of tasks with greater efficiency and adaptability, unlocking their full potential across diverse applications. 

2.6 Knowledge-Based Reasoning and Generation 

Automatic Reasoning and Tool-use (ART) 

The limited reasoning abilities and lack of external tool utiliza-tion hinder the potential of LLMs in complex tasks. [ Paranjape 

et al. , 2023 ] introduced Automatic Reasoning and Tool-use (ART) to tackle this critical barrier that empowers LLMs to reason through multi-step processes and seamlessly integrate external expertise. ART bridges the reasoning gap, enabling LLMs to tackle complex problems and expand beyond simple text generation. By integrating external tools for specialized knowledge and computations, ART unlocks unprecedented versatility and informs LLM outputs with real-world relevance. This allows LLMs to contribute to diverse fields like scientific research, data analysis, and even decision-making support. Moving beyond traditional prompting techniques, ART au-tomates reasoning steps through structured programs, elimi-nating the need for laborious hand-crafting. Its dynamic tool integration ensures smooth collaboration, pausing generation to incorporate external tool outputs and seamlessly resuming the flow. Empirical evidence on challenging benchmarks (Big-Bench and MMLU) demonstrates ARTâ€™s effectiveness, sur-passing traditional prompting and even matching hand-crafted demonstrations in some cases. 

2.7 Improving Consistency and Coherence 

Contrastive Chain-of-Thought (CCoT) Prompting 

Traditional CoT prompting for LLMs often misses a crucial element: learning from mistakes. That is where Contrastive Chain-of-Thought Prompting (CCoT) [ Chia et al. , 2023 ] dives in, providing both valid and invalid reasoning demonstrations alongside original prompts. Imagine exploring a map with the right path and the wrong turns to avoid â€“ that is the advantage of contrastive CoT! This dual-perspective approach, tested on reasoning benchmarks like SQuAD and COPA, pushes LLMs to step-by-step reasoning, leading to 4-16% improvements in strategic and mathematical reasoning evaluations compared to traditional CoT, further improved by approximately 5% when integrated with self-consistency techniques. However, questions remain about this technique, such as the automated generation of contrasting demonstrations for diverse problems and its applicability to other NLP tasks beyond reasoning. 

2.8 Managing Emotions and Tone 

Emotion Prompting 

While LLMs demonstrate impressive capabilities on various tasks, their ability to comprehend psychological and emotional cues remains uncertain. The study by [Li et al. , 2023a ] ad-dressed the uncertainty surrounding LLMsâ€™ ability to compre-hend emotional cues by introducing EmotionPrompt. Drawing inspiration from psychological research on languageâ€™s impact on human performance, they append 11 emotional stimulus sentences to prompts to enhance LLM emotional intelligence. Experimental results demonstrate seamless integration of these stimuli, significantly improving LLM performance across var-ious tasks. EmotionPrompt demonstrates an 8.00% relative improvement in instruction induction and an impressive 115% boost in BIG-Bench tasks, underscoring its efficacy in aug-menting LLM capabilities in processing affective signals. An evaluation involving 106 participants reveals an average im-provement of 10.9% in performance, truthfulness, and respon-sibility metrics for generative tasks when employing Emotion-Prompt compared to standard prompts. 

2.9 Code Generation and Execution 

Scratchpad Prompting 

Despite the prowess of Transformer-based language models in generating code for basic programming tasks, they encounter challenges in complex, multi-step algorithmic calculations re-quiring precise reasoning. Addressing this, [Nye et al. , 2021 ]introduce a novel approach, centered on task design rather than model modification, introduce a â€˜scratchpadâ€™ concept. The pro-posal enables the model to generate an arbitrary sequence of in-termediate tokens before providing the final answer. Scratchpad Prompting technique outperforms (Mostly Basic Python Pro-gramming) MBPP-aug with a 46.8% success rate. Combining CodeNet and single-line datasets yields the highest performance, achieving 26.6% correct final outputs and 24.6% perfect traces. Scratchpad prompting technique faces limitations, including a fixed context window size of 512 tokens and a dependency on supervised learning for scratchpad utilization. 

Program of Thoughts (PoT) Prompting 

Language models are suboptimal for solving mathematical ex-pressions due to their proneness to arithmetic errors, incapa-bility in handling complex equations, and inefficiency in ex-pressing extensive iterations. To enhance numerical reasoning in language models, [Chen et al. , 2022 ] presents Program-of-Thoughts (PoT) prompting, advocating the use of external language interpreters for computation steps. PoT enables mod-els like Codex to express reasoning through executable Python programs, resulting in an average performance improvement of approximately 12% compared to CoT prompting on datasets involving mathematical word problems and financial questions. 

Structured Chain-of-Thought (SCoT) Prompting 

LLMs have exhibited impressive proficiency in code gener-ation. The widely used CoT prompting involves producing intermediate natural language reasoning steps before generat-ing code. Despite its efficacy in natural language generation, CoT prompting demonstrates lower accuracy when applied to code generation tasks. [ Li et al. , 2023c ] introduce Struc-tured Chain-of-Thought (SCoTs) as an innovative prompting technique tailored specifically for code generation. By in-corporating program structures (sequence, branch, and loop structures) into reasoning steps, SCoT prompting enhances LLMsâ€™ performance in generating structured source code. This approach explicitly guides LLMs to consider requirements from the source code perspective, improving their overall ef-fectiveness in code generation compared to CoT prompting. The authors validated the effectiveness of SCoT on ChatGPT and Codex across three benchmarks (HumanEval, MBPP, and MBCPP) and demonstrated a superior performance over the CoT prompting by up to 13.79%. 

Chain-of-Code (CoC) Prompting 

While CoT prompting has proven very effective for enhancing Language models (LMs) semantic reasoning skills, it strug-gles to handle questions requiring numeric or symbolic reason-ing. [ Li et al. , 2023b ] introduce Chain-of-Code (CoC) as an extension to improve LM reasoning by leveraging codewriting for both logic and semantic tasks. CoC encourages LMs to format semantic sub-tasks as flexible pseudocode, allowing an interpreter to catch undefined behaviors and simulate them with an "LMulator." Experiments demonstrate CoCâ€™s superior-ity over Chain of Thought and other baselines, achieving an 84% accuracy on BIG-Bench Hard, a 12% gain. CoC proves effective with both large and small models, expanding LMsâ€™ ability to correctly answer reasoning questions by incorporat-ing a "think in code" approach. 

2.10 Optimization and Efficiency 

Optimization by Prompting (OPRO) 

In various domains, optimization is a fundamental process often involving iterative techniques. [Yang et al. , 2023 ] intro-Table 1: Summary of prevalent prompting techniques of LLMs based on the following factors: application, prompt acquisition, prompt turn, language model, dataset, and metrics.                                                                                                                                                                      

> Application Prompting Technique Comparison Scope Prompt Acquisition Prompt Turn Language Model(s) Dataset Metric(s)
> New Tasks Without Training Data Zero-shot Manual Single GPT-2 Arithmetic,Symbolic Accuracy, ROUGE Score Few-shot Manual Single GPT-3 NaturalQS, WebQS, TriviaQA Accuracy CoT Manual Multi PaLM 540B GSM8K Accuracy LogiCoT Manual Multi Vicuna-33b, GPT-4 GSM8K, AQuA, SocialQA Accuracy CoS Manual Multi gpt-3.5-turbo , GPT-4 SPARTUN Accuracy, Precision, Recall Auto-CoT LM Generated Multi GPT-3 Arithmetic, Symbolic Accuracy Self-Consistency Manual Single PaLM 540B Arithmetic, Commonsense Accuracy Reasoning and Logic ToT Retrieval Based Multi GPT-4 Game of 24, Creative Writing Success Rate GoT Retrieval Based Multi T5-large GSM8K, ScienceQA ROUGE Score S2A Manual Single Llama 2-70B QA,GSM8K Accuracy ThoT Hybrid Multi gpt-3.5-turbo , Llama 2-70b-chat PopQA, EntityQ, MTCR Exact Match (EM) Score Chain of Table Manual Multi GPT 3.5, LLaMA 2 TabFact, WikiTQ BLEU, ROUGE Score CoVe Retrieval Based Multi Llama 65B Wikidata, QUEST, MultiSpanQA Precision, F1 ReAct Retrieval Based Multi PaLM-540B, GPT-3 HotpotQA, FEVER Exact Match (EM), Accuracy Reduce Hallucination RAG Retrieval Based Single RAG-Token, RAG-Seq. MSMARCO, SearchQA ROUGE, BLEU score CoN LM Generated Multi Llama 2, DPR NQ, TriviaQA, WebQ Exact Match (EM), F1 Score CoK LM Generated Multi gpt-3.5-turbo-0613 HotpotQA, FEVER, MedMCQA, MMLU Physics and Biology Exact Match (EM), Accuracy User Interaction Active-Prompt Manual Single code-davinci-002 ,text-davinci-003 Arithmetic, Commonsense, Symbolic Disagreement, Entropy Variance, Self-confidence Score Fine-Tuning and Optimization APE LM Generated Single text-curie-001 ,text-davanci-002 BBII, TruthfulQA Execution accuracy, Log probability, Efficient score estimation Knowledge-Based Reasoning and Generation ART Hybrid Multi GPT-3 (175B) BigBench, MMLU Accuracy Improving Consistency and Coherence CCoT LM Generated Multi gpt-3.5-turbo-0301 Arithmetic, Factual QA Accuracy Managing Emotions and Tone Emotion Prompting Manual Single GPT-4 BIG-Bench, Instruction Induction Accuracy SCoT Hybrid Multi ChatGPT, Codex HumanEval, MBPP, MBCPP pass@k Code Generation and Execution PoT Manual Single gpt-3.5-turbo GSM8K, SVAMP, FinQA Exact Match(EM) Score CoC Manual Single text-davinci-003 ,gpt-3.5-turbo BIG-Bench Hard Accuracy Scratchpad Prompting Manual Single GPT-3 MBPP, MBPP-aug Accuracy Optimization and Efficiency OPRO Manual Single PaLM 2-L-IT, text-bison GSM8K, BIG-Bench Hard Accuracy Understanding User Intent RaR Manual Single GPT-4-061 3Knowledge, Symbolic Accuray, Fair Score, Language Modeling Score Metacognition and Self-Reflection Take a Step Back Manual Single PaLM2-L, GPT-4 MMLU-Physics, MMLU-Chemistry TimeQA, SituatedQA, StrategyQA Accuracy

duce Optimization by PROmpting (OPRO), a novel approach that leverages LLMs as optimizers. Unlike traditional meth-ods, OPRO utilizes natural language prompts to iteratively generate solutions based on the problem description, enabling quick adaptation to different tasks and customization of the optimization process. The potential of LLMs for optimization is demonstrated through case studies on classic problems like linear regression and the traveling salesman problem. Addi-tionally, it explores the optimization of prompts to maximize accuracy in natural language processing tasks, highlighting the sensitivity of LLMs. The experiments show that optimiz-ing prompts for accuracy on a small training set effectively translates to high performance on the test set. OPRO leads to a significant performance boost, with the most effective prompts optimized by OPRO outperforming human-designed prompts by up to 8% on the GSM8K dataset and up to 50% on challenging tasks in Big-Bench. 

2.11 Understanding User Intent 

Rephrase and Respond (RaR) Prompting 

The study by [Deng et al. , 2023 ] brings attention to an often-neglected dimension in exploring LLMs: the disparity between human thought frames and those of LLMs and introduces Rephrase and Respond (RaR). RaR allows LLMs to rephrase and expand questions in a single prompt, demonstrating im-proved comprehension and response accuracy. The two-step RaR variant, incorporating rephrasing and response LLMs, achieves substantial performance enhancements across vari-ous tasks. The study highlights that in contrast to casually posed human queries, the rephrased questions contribute to en-hanced semantic clarity and the resolution of inherent ambiguity. These findings offer valuable insights for understanding and enhancing the efficacy of LLMs across various applications. 

2.12 Metacognition and Self-Reflection 

Take a Step Back Prompting 

Addressing the persistent challenge of complex multi-step rea-soning, [Zheng et al. , 2023 ] introduced the Step-Back prompt-ing technique, tailored explicitly for advanced language models like PaLM-2L. This innovative approach empowers models to engage in abstraction, extracting high-level concepts and fun-damental principles from specific instances. The Step-Back prompting method involves a two-step procedure, integrating Abstraction and Reasoning. Through extensive experiments, ap-plying Step-Back Prompting to PaLM-2L in diverse reasoning-intensive tasks such as STEM, Knowledge QA, and Multi-Hop Reasoning, the results demonstrate a substantial enhancement in reasoning capabilities. Noteworthy performance boosts are observed, with improvements in tasks like MMLU Physics and Chemistry by 7%, TimeQA by 27%, and MuSiQue by 7%. 

3 Conclusion 

In the domain of artificial intelligence, prompt engineering has become a transformative force, unlocking the vast potential of LLMs. This survey paper aims to serve as a foundational resource that systematically categorizes 29 distinct prompt engi-neering techniques based on their targeted functionalities, inspir-ing further research and empowering innovators in the evolving landscape of prompt engineering. The analysis spans applica-tions, models, and datasets, shedding light on the strengths and limitations of each approach. Furthermore, we have added a diagram and a table to highlight the important points. Despite the remarkable successes, challenges persist, including biases, factual inaccuracies, and interpretability gaps, necessitating further investigation and mitigation strategies. The future of prompt engineering holds immense potential, with emerging trends like meta-learning and hybrid prompting architectures promising amplified capabilities. However, ethical considera-tions are paramount, emphasizing responsible development and deployment to ensure positive integration into our lives. 

References 

[Bahng et al. , 2022] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large-scale models. arXiv preprint arXiv:2203.17274 , 2022. [Brown et al. , 2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-pher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [Chen et al. , 2022] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 , 2022. [Chen et al. , 2023] Banghao Chen, Zhaofeng Zhang, Nicolas LangrenÃ©, and Shengxin Zhu. Unleashing the potential of prompt engineering in large language models: a compre-hensive review. arXiv preprint arXiv:2310.14735 , 2023. [Chia et al. , 2023] Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-thought prompting. arXiv preprint arXiv:2311.09277 ,2023. [Deng et al. , 2023] Yihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205 , 2023. [Dhuliawala et al. , 2023] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Ce-likyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495 , 2023. [Diao et al. , 2023] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-of-thought for large language models. arXiv preprint arXiv:2302.12246 , 2023. [Hu et al. , 2023] Hanxu Hu, Hongyuan Lu, Huajian Zhang, Yun-Ze Song, Wai Lam, and Yue Zhang. Chain-of-symbol prompting elicits planning in large langauge models, 2023. [Lewis et al. , 2020] Patrick Lewis, Ethan Perez, Aleksan-dra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Infor-mation Processing Systems , 33:9459â€“9474, 2020. [Li et al. , 2023a] Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. Large language models understand and can be enhanced by emotional stimuli. arXiv preprint arXiv:2307.11760 , 2023. [Li et al. , 2023b] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented code emulator. arXiv preprint arXiv:2312.04474 , 2023. [Li et al. , 2023c] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. Structured chain-of-thought prompting for code generation. 

arXiv preprint arXiv:2305.06599 , 2023. [Li et al. , 2023d] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Li-dong Bing. Chain-of-knowledge: Grounding large lan-guage models via dynamic knowledge adapting over het-erogeneous sources, 2023. [Liu et al. , 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys , 55(9):1â€“35, 2023. [Long, 2023] Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291 , 2023. [Nye et al. , 2021] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for inter-mediate computation with language models. arXiv preprint arXiv:2112.00114 , 2021. [Paranjape et al. , 2023] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014 , 2023. [Radford et al. , 2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. Ope-nAI blog , 1(8):9, 2019. [Tonmoy et al. , 2024] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. A comprehensive survey of hallucination mitiga-tion techniques in large language models. arXiv preprint arXiv:2401.01313 , 2024. [Wang et al. , 2022] Xuezhi Wang, Jason Wei, Dale Schuur-mans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowd-hery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022. [Wang et al. , 2024] Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. Chain-of-table: Evolving tables in the reasoning chain for table understanding, 2024. [Wei et al. , 2022] Jason Wei, Xuezhi Wang, Dale Schuur-mans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824â€“24837, 2022. [Weston and Sukhbaatar, 2023] Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829 , 2023. [Wu et al. , 2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual founda-tion models, 2023. [Yang et al. , 2023] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. arXiv preprint arXiv:2309.03409 , 2023. [Yao et al. , 2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. Re-act: Synergizing reasoning and acting in language models. 

arXiv preprint arXiv:2210.03629 , 2022. [Yao et al. , 2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601 , 2023. [Yao et al. , 2023b] Yao Yao, Zuchao Li, and Hai Zhao. Be-yond chain-of-thought, effective graph-of-thought rea-soning in large language models. arXiv preprint arXiv:2305.16582 , 2023. [Yu et al. , 2023] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-of-note: Enhancing robustness in retrieval-augmented lan-guage models, 2023. [Zhang et al. , 2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493 ,2022. [Zhao et al. , 2023] Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, and Stefan Wermter. Enhancing zero-shot chain-of-thought reason-ing in large language models through logic, 2023. [Zheng et al. , 2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou. Take a step back: evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117 , 2023. [Zhou et al. , 2022] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910 , 2022. [Zhou et al. , 2023] Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, and Jian-bing Shen. Thread of thought unraveling chaotic contexts. 

arXiv preprint arXiv:2311.08734 , 2023.
